{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"About Parareal in Python","text":"<p>This package implements the Parareal algorithm in Python. Parareal is an algorithm for parallel-in-time integration, meaning that it attempts to parallelize the otherwise completely sequential time integration of an ordinary differential equation (ODE). The algorithm works by using two methods to solve the ODE: a cheap and coarse method and a more computationally intensive fine method. This will only work if the coarse method is good enough so that we can initiate multiple fine integrations from a coarsely estimated intermediate condition.</p> <p>The body of literature on Parareal is quite extensive. The original method was described by Lions et al. in 2001 12.</p> <p>This Python module implements Parareal as a so-called black-box method. This means that the algorithm doesn't need to know about the details of the simulation. The user writes a script containing a fine and a coarse integrator. The <code>parareal</code> module then manages the computation using <code>dask</code>.</p> <p>We have three examples that serve as user documentation.</p> <ol> <li>Dampened harmonic oscillator (simple version)</li> <li>Using HDF5 and MPI (still with harmonic oscillator example)</li> <li>Pipe flow using OpenFOAM</li> </ol> <p>The library is implemented using literate programming, see Implementation.</p> <ol> <li> <p>Jacques Louis Lions, Yvon Maday, and Gabriel Turinici. R\u00e9solution d'EDP par un sch\u00e9ma en temps \u00abparar\u00e9el\u00bb. Comptes Rendus de l'Acad\u00e9mie des Sciences - Series I - Mathematics, 332(7):661\u2013668, apr 2001. doi:10.1016/S0764-4442(00)01793-6.\u00a0\u21a9</p> </li> <li> <p>Eric Aubanel. Scheduling of tasks in the parareal algorithm. Parallel Computing, 37(3):172\u2013182, 2011. URL: https://www.sciencedirect.com/science/article/pii/S0167819110001419, doi:10.1016/j.parco.2010.10.004.\u00a0\u21a9</p> </li> </ol>"},{"location":"01-dho-simple/","title":"Tutorial: Dampened Harmonic Oscillator","text":"<p>The <code>parareal</code> module takes a rather principled approach, where all mathematical concepts are backed by types in Python's nascent static type (annotation) system.</p>"},{"location":"01-dho-simple/#glossary","title":"Glossary","text":"<p>We may present the Parareal algorithm in abstract terms, and match those terms with corresponding type definitions in Python.</p> <p>We need to define the following:</p> Vector A <code>Vector</code> is an object that represents the state of a solution at any one time. On this state we need to be able to do addition, subtraction and scalar multiplication, in order to perform the Parareal algorithm. Solution A <code>Solution</code> is a function that takes an initial <code>Vector</code>, a time <code>t_0</code> and a time <code>t</code>, returning the state <code>Vector</code> at time <code>t</code>. Mapping A <code>Mapping</code> is a function from one state <code>Vector</code> to another, for example a mapping from a coarse to a fine mesh or vice-versa. Fine Solution The fine solution is the solution at the desired resolution. If we were not doing parallel-in-time, this would be the integrator to get at the correct result. We may also use the fine solution to find a ground thruth in testing the Parareal solution. Coarse Solution The coarse solution is the solution that is fast but less accurate. parareal/abstract.py<pre><code>from __future__ import annotations\nfrom typing import (Callable, Protocol, TypeVar, Union)\n\n&lt;&lt;abstract-types&gt;&gt;\n</code></pre>"},{"location":"01-dho-simple/#vector","title":"Vector","text":"<p>We have an ODE in the form</p> \\[y' = f(y, t).\\] <p>Here \\(y\\) can be a scalar value, a vector of values (say a <code>numpy</code> array), or any expression of state. A naive implementation of an ODE integrator would be</p> \\[y_{n+1} = y_{n} + \\Delta t f(y_{n}, t),\\] <p>also known as the forward Euler method. We can capture the state \\(y\\) in an abstract class we'll call <code>Vector</code>. We chose this name because we expect this objects to share (some of) the arithmetic properties of mathematical vectors. Namely, we want to be able to add, subtract and scale them. The chunk below states this need of a basic arithmetic in the form of abstract methods.</p> #abstract-types<pre><code>TVector = TypeVar(\"TVector\", bound=\"Vector\")\n\nclass Vector(Protocol):\n    def __add__(self: TVector, other: TVector) -&gt; TVector:\n        ...\n\n    def __sub__(self: TVector, other: TVector) -&gt; TVector:\n        ...\n\n    def __mul__(self: TVector, other: float) -&gt; TVector:\n        ...\n\n    def __rmul__(self: TVector, other: float) -&gt; TVector:\n        ...\n</code></pre> <p>We don't actually need to implement these methods right now. All this is saying, is that any type that has these methods defined can stand in for a <code>Vector</code>.</p> <p>Note that we don't make a formal distinction here between a state vector and a vector representing a change in state.</p> <p>Suppose we have a <code>Vector</code> type \\(T\\), a <code>Mapping</code> is a function \\(T \\to T\\):</p> #abstract-types<pre><code>Mapping = Callable[[TVector], TVector]\n</code></pre>"},{"location":"01-dho-simple/#problem","title":"Problem","text":"<p>An ODE is then given as a function taking a <code>Vector</code> (the state \\(y\\)) and a <code>float</code> (the time \\(t\\)) returning a <code>Vector</code> (the derivative \\(y' = f(y,t)\\) evaluated at \\((y,t)\\)). We define the type <code>Problem</code>:</p> #abstract-types<pre><code>Problem = Callable[[TVector, float], TVector]\n</code></pre> <p>In mathematical notation the snippet above means:</p> \\[{\\rm Problem} : (y, t) \\to f(y, t) = y'\\]"},{"location":"01-dho-simple/#solution","title":"Solution","text":"<p>If we have a <code>Problem</code>, we're after a <code>Solution</code>: a function that, given an initial <code>Vector</code> (the initial condition \\(y_0\\)), initial time (\\(t_0\\)) and final time (\\(t\\)), gives the resulting <code>Vector</code> (the solution, \\(y(t)\\) for the given initial conditions).</p> #abstract-types<pre><code>Solution = Union[Callable[[TVector, float, float], TVector],\n                 Callable[..., TVector]]\n</code></pre> <p>Those readers more familiar with classical physics or mathematics may notice that our <code>Problem</code> object corresponds with the function \\(f\\). The <code>Solution</code> object, on the other hand, corresponds with the evolution operator \\(\\phi\\):</p> \\[{\\rm Solution} : (y_0, t_0; t) \\to \\phi(y_0, t_0; t) = y.\\] <p>Intuitively, \\(\\phi\\) represents any method that solves (even approximately) our initial value problem.</p>"},{"location":"01-dho-simple/#example","title":"Example","text":"<p>An example of a <code>Problem</code> would be the function,</p> \\[f(y, t) = r y,\\] <p>in which case the corresponding <code>Solution</code> is,</p> \\[\\phi(y_0, t_0; t) = y_0 e^{r(t - t_0)}.\\]"},{"location":"01-dho-simple/#solver","title":"Solver","text":"<p>The challenge is, of course, to find a way of transforming a <code>Problem</code> into a <code>Solution</code>. This is what integration algorithms, or solvers do:</p> \\[{\\rm Solver} : {\\rm Problem} \\to {\\rm Solution}.\\] <p>If we look a bit closely at the definitions of <code>Problem</code> and <code>Solution</code> we'll notice that a solver is indeed a functional that accepts functions of \\((y,t)\\) as an input and returns functions of \\((y_0, t_0, t)\\) as an output.</p> <p>An example of such a solver is the forward Euler method, that can be implemented as:</p> parareal/forward_euler.py<pre><code>from .abstract import (Vector, Problem, Solution)\n\ndef forward_euler(f: Problem) -&gt; Solution:\n\"\"\"Forward-Euler solver.\"\"\"\n    def step(y: Vector, t_0: float, t_1: float) -&gt; Vector:\n\"\"\"Stepping function of Euler method.\"\"\"\n        return y + (t_1 - t_0) * f(y, t_0)\n    return step\n</code></pre> <p>Any existing solution can be iterated over to provide a solution over a larger time interval. The <code>iterate_solution</code> function runs a given solution with a step-size fixed to \\(\\Delta t = h\\).</p> parareal/iterate_solution.py<pre><code>from .abstract import (Vector, Solution)\nimport numpy as np\nimport math\n\ndef iterate_solution(step: Solution, h: float) -&gt; Solution:\n    def iter_step(y: Vector, t_0: float, t_1: float) -&gt; Vector:\n\"\"\"Stepping function of iterated solution.\"\"\"\n        n = math.ceil((t_1 - t_0) / h)\n        steps = np.linspace(t_0, t_1, n + 1)\n        for t_a, t_b in zip(steps[:-1], steps[1:]):\n            y = step(y, t_a, t_b)\n        return y\n    return iter_step\n</code></pre>"},{"location":"01-dho-simple/#numeric-solution","title":"Numeric solution","text":"<p>To plot a <code>Solution</code>, we need to tabulate the results for a given sequence of time points.</p> parareal/tabulate_solution.py<pre><code>from .abstract import (Solution, Vector)\nfrom typing import (Sequence, Any)\nimport numpy as np\n\nArray = Any\n\ndef tabulate(step: Solution, y_0: Vector, t: Array) -&gt; Sequence[Vector]:\n\"\"\"Tabulate the step-wise solution, starting from `y_0`, for every time\n    point given in array `t`.\"\"\"\n    if isinstance(y_0, np.ndarray):\n        return tabulate_np(step, y_0, t)\n\n    y = [y_0]\n    for i in range(1, t.size):\n        y_i = step(y[i-1], t[i-1], t[i])\n        y.append(y_i)\n    return y\n\n&lt;&lt;tabulate-np&gt;&gt;\n</code></pre> Numpy specialisation of `tabulate` In the case that the `Vector` type is actually a numpy array, we can specialize the `tabulate` routine to return a larger array.  #tabulate-np<pre><code>def tabulate_np(step: Solution, y_0: Array, t: Array) -&gt; Array:\n    y = np.zeros(dtype=y_0.dtype, shape=(t.size,) + y_0.shape)\n    y[0] = y_0\n    for i in range(1, t.size):\n        y[i] = step(y[i-1], t[i-1], t[i])\n    return y\n</code></pre>"},{"location":"01-dho-simple/#theory","title":"Theory","text":"<p>The harmonic oscillator can model the movement of a pendulum or the vibration of a mass on a string.</p> \\[y'' + 2\\zeta \\omega_0 y' + \\omega_0^2 y = 0,\\] <p>where \\(\\omega_0 = \\sqrt{k/m}\\) and \\(\\zeta = c / 2\\sqrt{mk}\\), \\(k\\) being the spring constant, \\(m\\) the test mass and \\(c\\) the friction constant.</p> <p>To solve this second order ODE we need to introduce a second variable to solve for. Say \\(q = y\\) and \\(p = y'\\).</p> \\[\\begin{aligned}     q' &amp;= p\\\\     p' &amp;= -2\\zeta \\omega_0 p + \\omega_0^2 q \\end{aligned}\\] <p>The <code>Problem</code> is then given as</p> #harmonic-oscillator-problem<pre><code>def harmonic_oscillator(omega_0: float, zeta: float) -&gt; Problem:\n    def f(y, t):\n        return np.r_[y[1], -2 * zeta * omega_0 * y[1] - omega_0**2 * y[0]]\n    return f\n</code></pre> build/harmonic_oscillator.py<pre><code>from parareal.abstract import (Problem)\nfrom typing import Callable\nfrom numpy.typing import NDArray\nimport numpy as np\n\n&lt;&lt;harmonic-oscillator-problem&gt;&gt;\n&lt;&lt;harmonic-oscillator-solution&gt;&gt;\n\nif __name__ == \"__main__\":\n    import numpy as np  # type: ignore\n    import pandas as pd  # type: ignore\n    from plotnine import ggplot, geom_line, aes  # type: ignore\n\n    from parareal.forward_euler import forward_euler\n    from parareal.iterate_solution import iterate_solution\n    from parareal.tabulate_solution import tabulate_np\n\n    OMEGA0 = 1.0\n    ZETA = 0.5\n    H = 0.001\n    system = harmonic_oscillator(OMEGA0, ZETA)\n\n    def coarse(y, t0, t1):\n        return forward_euler(system)(y, t0, t1)\n\n    # fine :: Solution[NDArray]\n    def fine(y, t0, t1):\n        return iterate_solution(forward_euler(system), H)(y, t0, t1)\n\n    y0 = np.array([1.0, 0.0])\n    t = np.linspace(0.0, 15.0, 100)\n    exact_result = underdamped_solution(OMEGA0, ZETA)(t)\n    euler_result = tabulate_np(fine, y0, t)\n\n    data = pd.DataFrame({\n        \"time\": t,\n        \"exact_q\": exact_result[:,0],\n        \"exact_p\": exact_result[:,1],\n        \"euler_q\": euler_result[:,0],\n        \"euler_p\": euler_result[:,1]})\n\n    plot = ggplot(data) \\\n        + geom_line(aes(\"time\", \"exact_q\")) \\\n        + geom_line(aes(\"time\", \"euler_q\"), color=\"#000088\")\n    plot.save(\"plot.svg\")\n</code></pre>"},{"location":"01-dho-simple/#exact-solution","title":"Exact solution","text":"<p>The damped harmonic oscillator has an exact solution, given the ansatz \\(y = A \\exp(z t)\\), we get</p> \\[z_{\\pm} = \\omega_0\\left(-\\zeta \\pm \\sqrt{\\zeta^2 - 1}\\right).\\] <p>and thus the general solution:</p> \\[y(t) = \\left\\{\\begin{array}{lr} A \\exp(z_+ t) + B \\exp(z_- t), &amp; {\\rm if}\\ \\zeta \\neq 1\\\\ (A + Bt) \\exp(-\\omega_0 t), &amp; {\\rm if}\\ \\zeta = 1\\end{array} \\right.\\] <p>This dynamical system has three qualitatively different solutions, each of them depending on the sign of the contents of the square root. Particularly, if the contents of the square root are negative, the two possible values for \\(z\\) will be complex numbers, making oscillations possible. More specifically, the three cases are:</p> <ul> <li>overdamped (\\(\\zeta &gt; 1\\) and, thus, both \\(z\\) are real numbers)</li> <li>critical dampening (\\(\\zeta = 1\\) and \\(z\\) is real and equal to \\(-\\omega_0\\))</li> <li>underdamped (\\(\\mid \\zeta \\mid &lt; 1\\), and \\(z = -\\omega_0\\zeta \\mp i \\omega_0 \\sqrt{1 - \\zeta^2}\\)).</li> </ul> <p>The underdamped case is typically the most interesting one. In this case we have solutions of the form:</p> \\[y = A\\quad \\underbrace{\\exp(-\\omega_0\\zeta t)}_{\\rm dampening}\\quad\\underbrace{\\exp(\\pm i \\omega_0 \\sqrt{1 - \\zeta^2} t)}_{\\rm oscillation},\\] <p>Given an initial condition \\(q_0 = 1, p_0 = 0\\), the solution is computed as</p> #harmonic-oscillator-solution<pre><code>def underdamped_solution(omega_0: float, zeta: float) \\\n        -&gt; Callable[[np.ndarray[np.float64]], np.ndarray[np.float64]]:\n    amp   = 1 / np.sqrt(1 - zeta**2)\n    phase = np.arcsin(zeta)\n    freq  = omega_0 * np.sqrt(1 - zeta**2)\n\n    def f(t: np.ndarray[np.float64]) -&gt; np.ndarray[np.float64]:\n        dampening = np.exp(-omega_0*zeta*t)\n        q = amp * dampening * np.cos(freq * t - phase)\n        p = - amp * omega_0 * dampening * np.sin(freq * t)\n        return np.c_[q, p]\n    return f\n</code></pre> Plot harmonic oscillator build/plot-harmonic-oscillator.py<pre><code>from typing import ( Callable )\nimport matplotlib.pylab as plt\nimport numpy as np\n\nfrom parareal.abstract import ( Problem )\nfrom parareal.forward_euler import ( forward_euler )\nfrom parareal.tabulate_solution import ( tabulate )\n\n&lt;&lt;harmonic-oscillator-problem&gt;&gt;\n&lt;&lt;harmonic-oscillator-solution&gt;&gt;\n\nomega_0 = 1.0\nzeta = 0.5\nf = harmonic_oscillator(omega_0, zeta)\nt = np.linspace(0.0, 15.0, 100)\ny_euler = tabulate(forward_euler(f), np.r_[1.0, 0.0], t)\ny_exact = underdamped_solution(omega_0, zeta)(t)\n\nplt.plot(t, y_euler[:,0], color='slateblue', label=\"euler\")\nplt.plot(t, y_exact[:,0], color='orangered', label=\"exact\")\nplt.plot(t, y_euler[:,1], color='slateblue', linestyle=':')\nplt.plot(t, y_exact[:,1], color='orangered', linestyle=':')\nplt.legend()\nplt.savefig(\"harmonic.svg\")\n</code></pre> #make-targets<pre><code>targets += harmonic.svg\n\nharmonic.svg: plot-harmonic-oscillator.py\n&gt; python plot-harmonic-oscillator.py\n</code></pre> <p>We can compare the results from the numeric integration with the exact solution.</p> <p></p>"},{"location":"01-dho-simple/#using-parareal","title":"Using Parareal","text":"examples/dho.py"},{"location":"02-dho-advanced/","title":"Tutorial: using HDF5 and MPI and Dask futures","text":"<p>In this example, we look again at the example of a dampened harmonic oscillator. This time we will go in engineering overkill mode and show how a model can be scaled to a large compute cluster using Dask, MPI and HDF5 as intermediate storage. To avoid confusion and difficult software configuration, we won't use the MPI feature of the HDF5 format. Instead, every job will write its output to its own HDF5 file.</p> <p>This tutorial covers the following concepts: * delayed arithmetic expressions * defining the coarse and fine integrators * using MPI with Dask</p>"},{"location":"02-dho-advanced/#best-practices","title":"Best practices","text":"<p>The following shows some best practices when working with orchestrated computations. One is about using well established data standards, the other about reducing overhead on the distributed scheduler. We can solve both these issues by abstracting over the representation of the state vector in our system. This technique is definitely overkill for our harmonic oscillator example, but it is also in general a good recipe for running Numpy based simulations in an organized manner.</p> examples/mpi_futures.py<pre><code>from __future__ import annotations\nimport argh  # type: ignore\nimport numpy as np\nfrom pathlib import Path\nfrom dataclasses import dataclass, field\nfrom typing import (Union, Callable, Optional, Any, Iterator)\nimport logging\n\n&lt;&lt;example-mpi-imports&gt;&gt;\n&lt;&lt;vector-expressions&gt;&gt;\n&lt;&lt;example-mpi-coarse&gt;&gt;\n&lt;&lt;example-mpi-fine&gt;&gt;\n&lt;&lt;example-mpi-history&gt;&gt;\n\ndef get_data(files: list[Path]) -&gt; Iterator[np.ndarray]:\n    for n in files:\n        with h5.File(n, \"r\") as f:\n            yield f[\"data\"][:]\n\ndef combine_fine_data(files: list[Path]) -&gt; np.ndarray:\n    data = get_data(files)\n    first = next(data)\n    return np.concatenate([first] + [x[1:] for x in data], axis=0)\n\ndef main(log: str = \"WARNING\", log_file: Optional[str] = None,\n         OMEGA0=1.0, ZETA=0.5, H=0.01):\n\"\"\"Run model of dampened hormonic oscillator in Dask\"\"\"\n    log_level = getattr(logging, log.upper(), None)\n    if not isinstance(log_level, int):\n        raise ValueError(f\"Invalid log level `{log}`\")\n    logging.basicConfig(level=log_level, filename=log_file)\n    &lt;&lt;example-mpi-main&gt;&gt;\n\nif __name__ == \"__main__\":\n    argh.dispatch_command(main)\n</code></pre>"},{"location":"02-dho-advanced/#vector-arithmetic-expressions","title":"Vector Arithmetic Expressions","text":""},{"location":"02-dho-advanced/#abstract-vectors","title":"Abstract vectors","text":"<p>It may be convenient to treat our <code>Vector</code> operations such that they are only performed once their output is needed. That way, we only need to schedule the actual integration steps as external jobs. In the meantime we have to store the arithmetic in a serializable <code>Expression</code> value. By doing this we reduce the amount of jobs that have to be handled by the scheduler, but also we reduce the amount of data that is being written and read from the file system.</p> <p>We will be using <code>functools.partial</code> and functions <code>operator.add</code>, <code>operator.mul</code> etc, to create a data structure that describes all the operations that we might do on a <code>Vector</code>. Results may be stored for reference in a <code>hdf5</code> file, a feature that can also be hidden behind our <code>Vector</code> interface.</p> #example-mpi-imports<pre><code>import operator\nfrom functools import partial\nimport h5py as h5  # type: ignore\nfrom abc import (ABC, abstractmethod)\n</code></pre> <p>We create a <code>Vector</code> class that satisfies the <code>Vector</code> concept outlined earlier. We store the operations in terms of unary and binary operators.</p> #vector-expressions<pre><code>class Vector(ABC):\n    @abstractmethod\n    def reduce(self: Vector) -&gt; np.ndarray:\n        pass\n\n    def __add__(self, other):\n        return BinaryExpr(operator.add, self, other)\n\n    def __sub__(self, other):\n        return BinaryExpr(operator.sub, self, other)\n\n    def __mul__(self, scale):\n        return UnaryExpr(partial(operator.mul, scale), self)\n\n    def __rmul__(self, scale):\n        return UnaryExpr(partial(operator.mul, scale), self)\n</code></pre> <p>The <code>Vector</code> class acts as a base class for the implementation of <code>BinaryExpr</code> and <code>UnaryExpr</code>, so that we can nest expressions accordingly. To force computation of a <code>Vector</code>, we supply the <code>reduce_expr</code> function that, in an example of terrible duck-typing, calls the <code>reduce</code> method recursively, until an object is reached that doesn't have the <code>reduce</code> method.</p> #vector-expressions<pre><code>def reduce_expr(expr: Union[np.ndarray, Vector]) -&gt; np.ndarray:\n    while isinstance(expr, Vector):\n        expr = expr.reduce()\n    return expr\n</code></pre>"},{"location":"02-dho-advanced/#hdf5-vectors","title":"HDF5 Vectors","text":"<p>This means we can also hide variables that are stored in an HDF5 file behind this interface. We often want to store more information than just the state vector. In the case of parareal, we have results from fine integration and coarse integration. In the case of fine integration, what we need to represent is the final state of the integration, but we are also interested in the intermediate steps.</p> #vector-expressions<pre><code>@dataclass\nclass H5Snap(Vector):\n    path: Path\n    loc: str\n    slice: list[Union[None, int, slice]]\n\n    def data(self):\n        with h5.File(self.path, \"r\") as f:\n            return f[self.loc].__getitem__(tuple(self.slice))\n\n    def reduce(self):\n        x = self.data()\n        logger = logging.getLogger()\n        logger.debug(f\"read {x} from {self.path}\")\n        return self.data()\n</code></pre> <p>To generate slices in a nice manner we can use a helper class:</p> #vector-expressions<pre><code>class Index:\n    def __getitem__(self, idx):\n        if isinstance(idx, tuple):\n            return list(idx)\n        else:\n            return [idx]\n\nindex = Index()\n</code></pre> <p>Then <code>index[a:b,c]</code> returns a list of slices <code>[slice(a,b), c]</code> (type <code>list[Union[None, int, slice]]</code>).</p>"},{"location":"02-dho-advanced/#operators","title":"Operators","text":"<p>There are two classes of operators, unary and binary (more arguments can usually be expressed as a composition of unary and binary forms). We store the arguments together with a function operating on the arguments. The function should be serializable (e.g. using <code>pickle</code>), meaning that <code>lambda</code> expressions are not allowed, but <code>partial</code> applications and functions in <code>operator</code> typically are ok.</p> #vector-expressions<pre><code>@dataclass\nclass UnaryExpr(Vector):\n    func: Callable[[np.ndarray], np.ndarray]\n    inp: Vector\n\n    def reduce(self):\n        a = reduce_expr(self.inp)\n        return self.func(a)\n\n\n@dataclass\nclass BinaryExpr(Vector):\n    func: Callable[[np.ndarray, np.ndarray], np.ndarray]\n    inp1: Vector\n    inp2: Vector\n\n    def reduce(self):\n        a = reduce_expr(self.inp1)\n        b = reduce_expr(self.inp2)\n        return self.func(a, b)\n</code></pre>"},{"location":"02-dho-advanced/#literal-expressions","title":"Literal expressions","text":"<p>To bootstrap our computation we may need to define a <code>Vector</code> directly represented by a Numpy array.</p> #vector-expressions<pre><code>@dataclass\nclass LiteralExpr(Vector):\n    value: np.ndarray\n\n    def reduce(self):\n        return self.value\n</code></pre>"},{"location":"02-dho-advanced/#the-coarse-and-fine-solvers","title":"The coarse and fine solvers","text":"<p>The API of <code>parareal</code> expects us to specify a solver with a function of three arguments, <code>y0</code>, <code>t0</code> and <code>t1</code>. For this function, we need the model input parameters to be in scope. Not only that, the scope needs to be completely serialised using <code>pickle</code>. For this reason, it is not enough to have a closure. We need to define a dataclass with a single <code>solution</code> method.</p> <p>The <code>Coarse</code> solution is not explicitely archived. We let <code>dask</code> handle the propagation of the result to further computations.</p> #example-mpi-coarse<pre><code>@dataclass\nclass Coarse:\n    n_iter: int\n    system: Any\n\n    def solution(self, y, t0, t1):\n        a = LiteralExpr(forward_euler(self.system)(reduce_expr(y), t0, t1))\n        logging.debug(f\"coarse result: {y} {reduce_expr(y)} {t0} {t1} {a}\")\n        return a\n</code></pre> <p>For the <code>fine</code> integrator however, we want to save the complete result, so that we can retrieve the full history after the computation has finished. So, instead of a <code>LiteralExpr</code>, the fine integrator returns a <code>H5Snap</code>.</p> #example-mpi-fine<pre><code>def generate_filename(name: str, n_iter: int, t0: float, t1: float) -&gt; str:\n    return f\"{name}-{n_iter:04}-{int(t0*1000):06}-{int(t1*1000):06}.h5\"\n\n@dataclass\nclass Fine:\n    parent: Path\n    name: str\n    n_iter: int\n    system: Any\n    h: float\n\n    def solution(self, y, t0, t1):\n        logger = logging.getLogger()\n        n = math.ceil((t1 - t0) / self.h)\n        t = np.linspace(t0, t1, n + 1)\n\n        self.parent.mkdir(parents=True, exist_ok=True)\n        path = self.parent / generate_filename(self.name, self.n_iter, t0, t1)\n\n        with h5.File(path, \"w\") as f:\n            logger.debug(\"fine %f - %f\", t0, t1)\n            y0 = reduce_expr(y)\n            logger.debug(\":    %s -&gt; %s\", y, y0)\n            x = tabulate(forward_euler(self.system), reduce_expr(y), t)\n            ds = f.create_dataset(\"data\", data=x)\n            ds.attrs[\"t0\"] = t0\n            ds.attrs[\"t1\"] = t1\n            ds.attrs[\"h\"] = self.h\n            ds.attrs[\"n\"] = n\n        return H5Snap(path, \"data\", index[-1])\n</code></pre>"},{"location":"02-dho-advanced/#keeping-track-of-convergence","title":"Keeping track of convergence","text":"<p>We want to cancel any scheduled computations as soon has we are happy that parareal has converged. This means we need to keep track of the results coming in, and check for convergence. This is done in the <code>History</code> class.</p> #example-mpi-history<pre><code>@dataclass\nclass History:\n    archive: Path\n    history: list[list[Vector]] = field(default_factory=list)\n\n    def convergence_test(self, y) -&gt; bool:\n        logger = logging.getLogger()\n        self.history.append(y)\n        if len(self.history) &lt; 2:\n            return False\n        a = np.array([reduce_expr(x) for x in self.history[-2]])\n        b = np.array([reduce_expr(x) for x in self.history[-1]])\n        maxdif = np.abs(a - b).max()\n        converged = maxdif &lt; 1e-4\n        logger.info(\"maxdif of %f\", maxdif)\n        if converged:\n            logger.info(\"Converged after %u iteration\", len(self.history))\n        return converged\n</code></pre>"},{"location":"02-dho-advanced/#dask-with-mpi","title":"Dask with MPI","text":"<p>There are two modes in which we may run Dask with MPI. One with a <code>dask-mpi</code> running as external scheduler, the other running everything as a single script. For this example we opt for the second, straight from the dask-mpi documentation:</p> #example-mpi-imports<pre><code>import dask\n# from dask_mpi import initialize  # type: ignore\nfrom dask.distributed import Client  # type: ignore\n</code></pre> #example-mpi-main<pre><code># initialize()\nclient = Client(n_workers=4, threads_per_worker=1)\n</code></pre>"},{"location":"02-dho-advanced/#running-the-harmonic-oscillator","title":"Running the harmonic oscillator","text":"#example-mpi-imports<pre><code>from parareal.futures import (Parareal)\n\nfrom parareal.forward_euler import forward_euler\nfrom parareal.tabulate_solution import tabulate\nfrom parareal.harmonic_oscillator import (underdamped_solution, harmonic_oscillator)\n\nimport math\n</code></pre> <p>For reference, we also run the full integrator using just the <code>Fine</code> solution.</p> #example-mpi-main<pre><code>system = harmonic_oscillator(OMEGA0, ZETA)\ny0 = np.array([1.0, 0.0])\nt = np.linspace(0.0, 15.0, 20)\narchive = Path(\"./output/euler\")\ntabulate(Fine(archive, \"fine\", 0, system, H).solution, LiteralExpr(y0), t)\n</code></pre>"},{"location":"02-dho-advanced/#running-parareal","title":"Running parareal","text":"#example-mpi-main<pre><code>archive = Path(\"./output/parareal\")\np = Parareal(\n    client,\n    lambda n: Coarse(n, system).solution,\n    lambda n: Fine(archive, \"fine\", n, system, H).solution)\njobs = p.schedule(LiteralExpr(y0), t)\nhistory = History(archive)\np.wait(jobs, history.convergence_test)\n\nclient.close()\n</code></pre>"},{"location":"02-dho-advanced/#convergence","title":"Convergence","text":"<p>The nice thing about the example with a dampened oscillator, is that we have a parameter by which we can tweak the amount of oscillations. Parareal is notoriously bad at oscillating behaviour, so we should see that reflected in the amount of iterations needed to converge.</p> <p> </p>"},{"location":"04-implementation/","title":"Implementation","text":"<pre><code># file=\"parareal/__init__.py\"\nimport subprocess\n\nfrom .tabulate_solution import tabulate\nfrom .parareal import parareal\nfrom . import abstract\n\n__all__ = [\"tabulate\", \"parareal\", \"schedule\", \"abstract\"]\n</code></pre>"},{"location":"04-implementation/#parareal","title":"Parareal","text":"<p>From Wikipedia:</p> <p>Parareal solves an initial value problem of the form</p> \\[\\dot{y}(t) = f(y(t), t), \\quad y(t_0) = y_0 \\quad \\text{with} \\quad t_0 \\leq t \\leq T.\\] <p>Here, the right hand side \\(f\\) can correspond to the spatial discretization of a partial differential equation in a method of lines approach. Parareal now requires a decomposition of the time interval \\([t_0, T]\\) into \\(P\\) so-called time slices \\([t_j, t_{j+1}]\\) such that</p> \\[[t_0, T] = [t_0, t_1] \\cup [t_1, t_2] \\cup \\ldots \\cup [t_{P-1}, t_{P} ].\\] <p>Each time slice is assigned to one processing unit when parallelizing the algorithm, so that \\(P\\) is equal to the number of processing units used for Parareal.</p> <p>Parareal is based on the iterative application of two methods for integration of ordinary differential equations. One, commonly labelled \\({\\mathcal {F}}\\), should be of high accuracy and computational cost while the other, typically labelled \\({\\mathcal {G}}\\), must be computationally cheap but can be much less accurate. Typically, some form of Runge-Kutta method is chosen for both coarse and fine integrator, where \\({\\mathcal {G}}\\) might be of lower order and use a larger time step than \\({\\mathcal {F}}\\). If the initial value problem stems from the discretization of a PDE, \\({\\mathcal {G}}\\) can also use a coarser spatial discretization, but this can negatively impact convergence unless high order interpolation is used. The result of numerical integration with one of these methods over a time slice \\([t_{j}, t_{j+1}]\\) for some starting value \\(y_{j}\\) given at \\(t_{j}\\) is then written as</p> \\[y = \\mathcal{F}(y_j, t_j, t_{j+1})\\ {\\rm or}\\ y = \\mathcal{G}(y_j, t_j, t_{j+1}).\\] <p>Serial time integration with the fine method would then correspond to a step-by-step computation of</p> \\[y_{j+1} = \\mathcal{F}(y_j, t_j, t_{j+1}), \\quad j=0, \\ldots, P-1.\\] <p>Parareal instead uses the following iteration</p> \\[y_{j+1}^{k+1} = \\mathcal{G}(y^{k+1}_j, t_j, t_{j+1}) + \\mathcal{F}(y^k_j, t_j, t_{j+1}) - \\mathcal{G}(y^k_j, t_j, t_{j+1}),\\\\ \\quad j=0, \\ldots, P-1, \\quad k=0, \\ldots, K-1,\\] <p>where \\(k\\) is the iteration counter. As the iteration converges and \\(y^{k+1}_j - y^k_j \\to 0\\), the terms from the coarse method cancel out and Parareal reproduces the solution that is obtained by the serial execution of the fine method only. It can be shown that Parareal converges after a maximum of \\(P\\) iterations. For Parareal to provide speedup, however, it has to converge in a number of iterations significantly smaller than the number of time slices, that is \\(K \\ll P\\).</p> <p>In the Parareal iteration, the computationally expensive evaluation of \\(\\mathcal{F}(y^k_j, t_j, t_{j+1})\\) can be performed in parallel on \\(P\\) processing units. By contrast, the dependency of \\(y^{k+1}_{j+1}\\) on \\(\\mathcal{G}(y^{k+1}_j, t_j, t_{j+1})\\) means that the coarse correction has to be computed in serial order.</p> <p>Don't get blinded by the details of the algorithm. After all, everything boils down to an update equation that uses a state vector \\(y\\) to calculate the state at the immediately next future step (in the same fashion as equation +@eq:euler-method did). The core equation translates to:</p> #parareal-core-1<pre><code>y_n[i] = coarse(y_n[i-1], t[i-1], t[i]) \\\n       + fine(y[i-1], t[i-1], t[i]) \\\n       - coarse(y[i-1], t[i-1], t[i])\n</code></pre> <p>If we include a <code>Mapping</code> between fine and coarse meshes into the equation, we get:</p> #parareal-core-2<pre><code>y_n[i] = c2f(coarse(f2c(y_n[i-1]), t[i-1], t[i])) \\\n       + fine(y[i-1], t[i-1], t[i]) \\\n       - c2f(coarse(f2c(y[i-1]), t[i-1], t[i]))\n</code></pre> <p>The rest is boiler plate. For the <code>c2f</code> and <code>f2c</code> mappings we provide a default argument of the identity function.</p> parareal/parareal.py<pre><code>from .abstract import (Solution, Mapping)\nimport numpy as np\n\ndef identity(x):\n    return x\n\ndef parareal(\n        coarse: Solution,\n        fine: Solution,\n        c2f: Mapping = identity,\n        f2c: Mapping = identity):\n    def f(y, t):\n        m = t.size\n        y_n = [None] * m\n        y_n[0] = y[0]\n        for i in range(1, m):\n            &lt;&lt;parareal-core-2&gt;&gt;\n        return y_n\n    return f\n\ndef parareal_np(\n        coarse: Solution,\n        fine: Solution,\n        c2f: Mapping = identity,\n        f2c: Mapping = identity):\n    def f(y, t):\n        m = t.size\n        y_n = np.zeros_like(y)\n        y_n[0] = y[0]\n        for i in range(1, m):\n            &lt;&lt;parareal-core-2&gt;&gt;\n        return y_n\n    return f\n</code></pre>"},{"location":"04-implementation/#running-in-parallel","title":"Running in parallel","text":"#import-dask<pre><code>from dask import delayed  # type: ignore\n</code></pre> #daskify<pre><code>&lt;&lt;import-dask&gt;&gt;\nimport numpy as np\n\nfrom pintFoam.parareal.harmonic_oscillator import \\\n    ( harmonic_oscillator, underdamped_solution )\nfrom pintFoam.parareal.forward_euler import \\\n    ( forward_euler )\nfrom pintFoam.parareal.tabulate_solution import \\\n    ( tabulate )\nfrom pintFoam.parareal.parareal import \\\n    ( parareal )\nfrom pintFoam.parareal.iterate_solution import \\\n    ( iterate_solution)\n\n\nattrs = {}\n\ndef green(f):\n    def greened(*args):\n        node = f(*args)\n        attrs[node.key] = {\"fillcolor\": \"#8888cc\", \"style\": \"filled\"}\n        return node\n    return greened\n\n@delayed\ndef gather(*args):\n    return list(args)\n</code></pre> <p>To see what Dask does, first we'll daskify the direct integration routine in <code>tabulate</code>. We take the same harmonic oscillator we had before. For the sake of argument let's divide the time line in three steps (so four points).</p> #daskify<pre><code>omega_0 = 1.0\nzeta = 0.5\nf = harmonic_oscillator(omega_0, zeta)\nt = np.linspace(0.0, 15.0, 4)\n</code></pre> <p>We now define the <code>fine</code> integrator:</p> #daskify<pre><code>h = 0.01\n\n@green\n@delayed\ndef fine(x, t_0, t_1):\n    return iterate_solution(forward_euler(f), h)(x, t_0, t_1)\n</code></pre> <p>It doesn't really matter what the fine integrator does, since we won't run anything. We'll just pretend. The <code>delayed</code> decorator makes sure that the integrator is never called, we just store the information that we want to call the <code>fine</code> function. The resulting value is a promise that at some point we will call the <code>fine</code> function. The nice thing is, that this promise behaves like any other Python object, it even qualifies as a <code>Vector</code>! The <code>tabulate</code> routine returns a <code>Sequence</code> of <code>Vector</code>s, in this case a list of promises. The <code>gather</code> function takes a list of promises and turns it into a promise of a list.</p> #daskify<pre><code>y_euler = tabulate(fine, [1.0, 0.0], t)\n</code></pre> <p>We can draw the resulting workflow:</p> build/plot-dask-seq.py<pre><code>&lt;&lt;daskify&gt;&gt;\n\ngather(*y_euler).visualize(\"docs/img/seq-graph.svg\", rankdir=\"LR\", data_attributes=attrs)\n</code></pre> <pre><code>Sequential integration\n---\n$(target): build/plot-dask-seq.py\n&gt; @mkdir -p $(@D)\n&gt; python $&lt;\n</code></pre> <p>This workflow is entirely sequential, every step depending on the preceding one. Now for Parareal! We also define the <code>coarse</code> integrator.</p> #daskify<pre><code>@delayed\ndef coarse(x, t_0, t_1):\n    return forward_euler(f)(x, t_0, t_1)\n</code></pre> <p>Parareal is initialised with the ODE integrated by the coarse integrator, just like we did before with the fine one.</p> #daskify<pre><code>y_first = tabulate(coarse, [1.0, 0.0], t)\n</code></pre> <p>We can now perform a single iteration of Parareal to see what the workflow looks like:</p> #daskify<pre><code>y_parareal = gather(*parareal(coarse, fine)(y_first, t))\n</code></pre> build/plot-dask-graphs.py<pre><code>&lt;&lt;daskify&gt;&gt;\n\ny_parareal.visualize(\"docs/img/parareal-graph.svg\", rankdir=\"LR\", data_attributes=attrs)\n</code></pre> <pre><code>Parareal iteration; the fine integrators (marked with blue squares) can be run in parallel.\n---\n$(target): build/plot-dask-graphs.py\n&gt; @mkdir -p $(@D)\n&gt; python $&lt;\n</code></pre>"},{"location":"04-implementation/#dask-futures","title":"Dask futures","text":"<p>We reimplement Parareal in the <code>futures</code> framework of Dask. We have a few helper functions: <code>identity</code> to be used as default instance for the mappings between coarse and fine meshes, and <code>pairs</code>, a function that iterates through successive pairs of a list.</p> parareal/futures.py<pre><code>from .abstract import (Solution, Mapping, Vector)\nfrom typing import (Callable)\nfrom dataclasses import dataclass\nfrom math import ceil\nimport numpy as np\nfrom numpy.typing import NDArray\nfrom dask.distributed import Client, Future  # type: ignore\nimport logging\n\n\ndef identity(x):\n    return x\n\ndef pairs(lst):\n    return zip(lst[:-1], lst[1:])\n\n&lt;&lt;parareal-futures&gt;&gt;\n</code></pre> <p>We need to send every operation to a remote worker, that includes summing the vectors from coarse and fine integrators.</p> #parareal-futures<pre><code>def combine(c1: Vector, f1: Vector, c2: Vector) -&gt; Vector:\n    return c1 + f1 - c2\n</code></pre> #time-windows<pre><code>def time_windows(times, window_size):\n\"\"\"Split the times vector in a set of time windows of a given size.\n\n    Args:\n        times:          The times vector\n        window_size:    The number of steps per window (note that n steps\n        correspond to n + 1 elements in the window). The last window may\n        be smaller.\n    \"\"\"\n    n_intervals = len(times) - 1\n    n = int(ceil(n_intervals / window_size))\n    m = window_size\n    return [times[i*m:min(i*m+m+1, len(times))] for i in range(n)]\n</code></pre> <p>Every call that actually requires some of the data needs to be sent to the remote worker(s). Where we could get away before with putting everything in a closure, now it is easier to make a class that includes the Dask <code>Client</code> instance.</p> #parareal-futures<pre><code>@dataclass\nclass Parareal:\n    client: Client\n    coarse: Callable[[int], Solution]\n    fine: Callable[[int], Solution]\n    c2f: Mapping = identity\n    f2c: Mapping = identity\n\n    def _c2f(self, x: Future) -&gt; Future:\n        if self.c2f is identity:\n            return x\n        return self.client.submit(self.c2f, x)\n\n    def _f2c(self, x: Future) -&gt; Future:\n        if self.f2c is identity:\n            return x\n        return self.client.submit(self.f2c, x)\n\n    def _coarse(self, n_iter: int, y: Future, t0: float, t1: float) -&gt;  Future:\n        logging.debug(\"Coarse run: %s, %s, %s\", y, t0, t1)\n        return self.client.submit(self.coarse(n_iter), y, t0, t1)\n\n    def _fine(self, n_iter: int, y: Future, t0: float, t1: float) -&gt; Future:\n        logging.debug(\"Fine run: %s, %s, %s\", y, t0, t1)\n        return self.client.submit(self.fine(n_iter), y, t0, t1)\n\n    &lt;&lt;parareal-methods&gt;&gt;\n</code></pre> <p>The <code>step</code> method implements the core parareal algorithm.</p> #parareal-methods<pre><code>def step(self, n_iter: int, y_prev: list[Future], t: NDArray[np.float64]) -&gt; list[Future]:\n    m = t.size\n    y_next = [None] * m\n    y_next[0] = y_prev[0]\n\n    for i in range(1, m):\n        c1 = self._c2f(self._coarse(n_iter, self.f2c(y_next[i-1]), t[i-1], t[i]))\n        f1 = self._fine(n_iter, y_prev[i-1], t[i-1], t[i])\n        c2 = self._c2f(self._coarse(n_iter, self.f2c(y_prev[i-1]), t[i-1], t[i]))\n        y_next[i] = self.client.submit(combine, c1, f1, c2)\n\n    return y_next\n</code></pre> <p>We schedule every possible iteration of parareal as a future. The tactic is to cancel remaining jobs only once we found a converging result. This way, workers can compute next iterations, even if the last step of the previous iteration is not yet complete and tested for convergence.</p> #parareal-methods<pre><code>def schedule(self, y_0: Vector, t: NDArray[np.float64]) -&gt; list[list[Future]]:\n    # schedule initial coarse integration\n    y_init = [self.client.scatter(y_0)]\n    for (a, b) in pairs(t):\n        y_init.append(self._coarse(0, y_init[-1], a, b))\n\n    # schedule all iterations of parareal\n    jobs = [y_init]\n    for n_iter in range(len(t)):\n        jobs.append(self.step(n_iter+1, jobs[-1], t))\n\n    return jobs\n</code></pre> <p>The <code>wait</code> method then gathers results and returns the first iteration that satisfies <code>convergence_test</code>.</p> #parareal-methods<pre><code>def wait(self, jobs, convergence_test):\n    for i in range(len(jobs)):\n        result = self.client.gather(jobs[i])\n        if convergence_test(result):\n            for j in jobs[i+1:]:\n                self.client.cancel(j, force=True)\n            return result\n    return result\n</code></pre>"},{"location":"04-implementation/#harmonic-oscillator","title":"Harmonic oscillator","text":"<p>We may test this on the harmonic oscillator.</p> test/test_futures.py<pre><code>from dataclasses import dataclass, field\nfrom functools import partial\nimport logging\nfrom numpy.typing import NDArray\nimport numpy as np\n\nfrom parareal.futures import Parareal\nfrom parareal.harmonic_oscillator import harmonic_oscillator\nfrom parareal.forward_euler import forward_euler\nfrom parareal.iterate_solution import iterate_solution\nfrom parareal.tabulate_solution import tabulate\n\nfrom dask.distributed import Client  # type: ignore\n\n\nOMEGA0 = 1.0\nZETA = 0.5\nH = 0.001\nsystem = harmonic_oscillator(OMEGA0, ZETA)\n\n\ndef coarse(_, y, t0, t1):\n    return forward_euler(system)(y, t0, t1)\n\n\ndef fine(_, y, t0, t1):\n    return iterate_solution(forward_euler(system), H)(y, t0, t1)\n\n\n@dataclass\nclass History:\n    history: list[NDArray[np.float64]] = field(default_factory=list)\n\n    def convergence_test(self, y):\n        self.history.append(np.array(y))\n        logging.debug(\"got result: %s\", self.history[-1])\n        if len(self.history) &lt; 2:\n            return False\n        return np.allclose(self.history[-1], self.history[-2], atol=1e-4)\n\n\ndef test_parareal():\n    client = Client()\n    p = Parareal(client, lambda n: partial(coarse, n), lambda n: partial(fine, n))\n    t = np.linspace(0.0, 15.0, 30)\n    y0 = np.array([0.0, 1.0])\n    history = History()\n    jobs = p.schedule(y0, t)\n    p.wait(jobs, history.convergence_test)\n\nif __name__ == \"__main__\":\n    logging.basicConfig(level=logging.DEBUG)\n    y0 = np.array([1.0, 0.0])\n    t = np.linspace(0.0, 15.0, 30)\n    result = tabulate(fine, y0, t)\n    print(result)\n</code></pre>"},{"location":"04-implementation/#building-figures","title":"Building figures","text":"build/Makefile<pre><code>.RECIPEPREFIX = &gt;\n\n.PHONY: all\n\n&lt;&lt;make-targets&gt;&gt;\n\nall: $(targets)\n</code></pre>"}]}